{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1wZkkQhQz5r",
        "outputId": "9541de4b-5a7a-49e8-ce7e-d5eeca4a4b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ],
      "source": [
        "!pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lxml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykQ_cz9nRHji",
        "outputId": "f51455e3-62d8-49b4-f4e7-e748d4e4c66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (4.9.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP_IGNIqRJkE",
        "outputId": "957fee32-3e84-4510-83db-7466d2a001d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests"
      ],
      "metadata": {
        "id": "sQ2-Eo8aRM66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate the right **HEADERS** dictionary for your web scraping use case, follow these steps:\n",
        "\n",
        "1. **Open Developer Tools in your web browser:** Go to the website you want to scrape and open the Developer Tools. Usually this can be done by right-clicking on the page and selecting \"Inspect\" or \"Inspect Element\".\n",
        "2. **Go to the Network Tab:** In the Developer Tools, find and click on the \"Network\" tab.\n",
        "3. **Reload the page:** Refresh the website while the Network tab is open. You will see a list of network requests being made.\n",
        "4. **Find the main request:** Look for the main request that loads the HTML content of the page. This is usually the first or one of the top requests in the list.\n",
        "5. **Inspect the Headers:** Click on the main request to see its details. Find the \"Headers\" tab within the request details.\n",
        "6. **Copy the User-Agent and Accept-Language:** Look for the \"User-Agent\" and \"Accept-Language\" headers and copy their values.\n",
        "7. **Create the HEADERS dictionary:** In your Python code, create a dictionary like the one in your example and paste the copied header values."
      ],
      "metadata": {
        "id": "tWU_OKRkSVzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(URL):\n",
        "    # opening our output file in append mode\n",
        "    File = open(\"out.csv\", \"a\")\n",
        "\n",
        "    # specifying user agent, You can use other user agents\n",
        "    # available on the internet\n",
        "    HEADERS = {\n",
        "        'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
        "        'Accept-Language': 'en-US, en;q=0.5'\n",
        "    }\n",
        "\n",
        "    # Making the HTTP Request\n",
        "    webpage = requests.get(URL, headers=HEADERS)\n",
        "\n",
        "    # Creating the Soup Object containing all data\n",
        "    soup = BeautifulSoup(webpage.content, \"lxml\")\n",
        "\n",
        "    # retrieving product title\n",
        "    try:\n",
        "        # Outer Tag Object\n",
        "        title = soup.find(\"span\", attrs={\n",
        "            'id': 'productTitle'\n",
        "        })\n",
        "\n",
        "        # Inner NavigableString Object\n",
        "        title_value = title.string\n",
        "\n",
        "        # Title as a string value\n",
        "        title_string = title_value.strip().replace(',', '')\n",
        "\n",
        "    except AttributeError:\n",
        "        title_string = \"NA\"\n",
        "    print(\"product Title = \", title_string)\n",
        "\n",
        "    # saving the title in the file\n",
        "    File.write(f\"{title_string},\")\n",
        "\n",
        "    # retrieving price\n",
        "    try:\n",
        "        price = soup.find(\"span\", attrs={\n",
        "            'id': 'priceblock_ourprice'\n",
        "        }).string.strip().replace(',', '')\n",
        "        # we are omitting unnecessary spaces\n",
        "        # and commas form our string\n",
        "    except AttributeError:\n",
        "        price = \"NA\"\n",
        "    print(\"Products price = \", price)\n",
        "\n",
        "    # saving\n",
        "    File.write(f\"{price},\")\n",
        "\n",
        "    # retrieving product rating\n",
        "    try:\n",
        "        rating = soup.find(\"i\", attrs={\n",
        "                           'class': 'a-icon a-icon-star a-star-4-5'\n",
        "        }).string.strip().replace(',', '')\n",
        "\n",
        "    except AttributeError:\n",
        "\n",
        "        try:\n",
        "            rating = soup.find(\"span\", attrs={\n",
        "                'class': 'a-icon-alt'\n",
        "            }).string.strip().replace(',', '')\n",
        "        except:\n",
        "            rating = \"NA\"\n",
        "    print(\"Overall rating = \", rating)\n",
        "\n",
        "    File.write(f\"{rating},\")\n",
        "\n",
        "    try:\n",
        "        review_count = soup.find(\"span\", attrs={\n",
        "            'id': 'acrCustomerReviewText'\n",
        "        }).string.strip().replace(',', '')\n",
        "\n",
        "    except AttributeError:\n",
        "        review_count = \"NA\"\n",
        "    print(\"Total reviews = \", review_count)\n",
        "    File.write(f\"{review_count},\")\n",
        "\n",
        "    # print availablility status\n",
        "    try:\n",
        "        available = soup.find(\"div\", attrs={'id': 'availability'})\n",
        "        available = available.find(\"span\").string.strip().replace(',', '')\n",
        "\n",
        "    except AttributeError:\n",
        "        available = \"NA\"\n",
        "    print(\"Availability = \", available)\n",
        "\n",
        "    # saving the availability and closing the line\n",
        "    File.write(f\"{available},\\n\")\n",
        "\n",
        "    # closing the file\n",
        "    File.close()"
      ],
      "metadata": {
        "id": "Ts7oOowDRdD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main(\"https://www.amazon.com/Dremel-Education-Accessories-Professional-Development/dp/B07KZ8XNDT\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIsFP8iKT_bj",
        "outputId": "6042e1eb-8f0f-420a-e380-9c5db1efe171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "product Title =  Dremel DigiLab 3D40-FLX-EDU 3D Printer Bundle - Flex Build Plate & Auto 9-Point Leveling - Includes 30 Lesson Plans Prof. Training Material & Extra Supplies - PC & MAC OS Chromebook iPad Compatible\n",
            "Products price =  NA\n",
            "Overall rating =  3.7 out of 5 stars\n",
            "Total reviews =  46 ratings\n",
            "Availability =  NA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e7OrRzPNUDVo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}